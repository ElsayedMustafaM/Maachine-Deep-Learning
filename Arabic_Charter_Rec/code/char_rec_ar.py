# -*- coding: utf-8 -*-
"""Char_Rec_Ar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_o8Ma9YlsojOMrOVmaays8o9F8qhPQ2r
"""

import numpy as np 
import matplotlib.pyplot as plt
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense,Flatten
from keras.layers import Dropout
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.optimizers import Adam
from keras.utils.np_utils import to_categorical
import random
import pandas as pd
from sklearn.preprocessing import LabelEncoder,OneHotEncoder

# Load the Drive helper and mount
from google.colab import drive

# This will prompt for authorization.
drive.mount('/content/drive')
#lists the content of your google drive
#!ls "/content/drive/My Drive"

# Read/Download MNIST Dataset
X_train= pd.read_csv('/content/drive/My Drive/TrainImages.csv')
y_train=pd.read_csv('/content/drive/My Drive/TrainLabel.csv')
X_test=pd.read_csv('/content/drive/My Drive/TestImages.csv')
y_test=pd.read_csv('/content/drive/My Drive/TestLabel.csv')

X_train=X_train.iloc[:,:].values.reshape((13439,32, 32))
X_test=X_test.iloc[:,:].values.reshape((3359,32, 32))
y_train=y_train.iloc[:,:].values
y_test=y_test.iloc[:,:].values
y_train.shape

assert(X_train.shape[0] == y_train.shape[0]), "The number of images is not equal to the number of labels."
assert(X_test.shape[0] == y_test.shape[0]), "The number of images is not equal to the number of labels."
assert(X_train.shape[1:] == (32,32)), "The dimensions of the images are not 28x28"
assert(X_test.shape[1:] == (32,32)), "The dimensions of the images are not 28x28"

#One Hot Encoding

labelencoder=LabelEncoder()
y_train[:,0]=labelencoder.fit_transform(y_train[:,0])
onehot=OneHotEncoder(categorical_features=[0])
y_train=onehot.fit_transform(y_train).toarray()

labelencoder=LabelEncoder()
y_test[:,0]=labelencoder.fit_transform(y_test[:,0])
onehot=OneHotEncoder(categorical_features=[0])
y_test=onehot.fit_transform(y_test).toarray()


#normalizaton
X_train=X_train/255
X_test=X_test/255


X_train=X_train.reshape(X_train.shape[0],32,32,1)
X_test=X_test.reshape(X_test.shape[0],32,32,1)
num_classes=28

def create_model():
  model=Sequential()
  model.add(Conv2D(30,(5,5),input_shape=(32,32,1),activation='relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Conv2D(15,(3,3),activation='relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Flatten())
  model.add(Dense(500,activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(num_classes,activation='softmax'))
  model.compile(Adam(lr=0.01),loss="categorical_crossentropy",metrics=['accuracy'])
  return model

model=create_model()
print(model.summary())

history=model.fit(X_train,y_train,validation_split=0.1,nb_epoch=10,batch_size=500,verbose=1,shuffle=1)

#visualize accuracy,loss


plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('loss')
plt.legend('loss')
plt.xlabel('epochs')

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('acc')
plt.legend('acc')
plt.xlabel('epochs')

#To evaluate the model 

score =model.evaluate(X_test,y_test)
print('Test score ' ,score[0])
print('Test accuracy ', score[1])

import pickle
filename = 'final_model.sav'
pickle.dump(model, open(filename, 'wb'))

from google.colab import files
files.download(filename)